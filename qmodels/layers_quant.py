import math
import warnings
from itertools import repeat
import collections.abc

import torch
from torch import nn
import torch.nn.functional as F
from typing import Callable, List, Optional, Tuple, Union

from .quantization_utils import Quantized_Linear, QuantAct, IntSoftmax, IntGELU, Quantizer, QuantMatMul


def _ntuple(n):
    def parse(x):
        if isinstance(x, collections.abc.Iterable):
            return x
        return tuple(repeat(x, n))

    return parse


to_2tuple = _ntuple(2)


def _no_grad_trunc_normal_(tensor, mean, std, a, b):
    # Cut & paste from PyTorch official master until it's in a few official releases - RW
    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf
    def norm_cdf(x):
        # Computes standard normal cumulative distribution function
        return (1.0 + math.erf(x / math.sqrt(2.0))) / 2.0

    if (mean < a - 2 * std) or (mean > b + 2 * std):
        warnings.warn(
            "mean is more than 2 std from [a, b] in nn.init.trunc_normal_. "
            "The distribution of values may be incorrect.",
            stacklevel=2,
        )

    with torch.no_grad():
        # Values are generated by using a truncated uniform distribution and
        # then using the inverse CDF for the normal distribution.
        # Get upper and lower cdf values
        l = norm_cdf((a - mean) / std)
        u = norm_cdf((b - mean) / std)

        # Uniformly fill tensor with values from [l, u], then translate to
        # [2l-1, 2u-1].
        tensor.uniform_(2 * l - 1, 2 * u - 1)

        # Use inverse cdf transform for normal distribution to get truncated
        # standard normal
        tensor.erfinv_()

        # Transform to proper mean, std
        tensor.mul_(std * math.sqrt(2.0))
        tensor.add_(mean)

        # Clamp to ensure it's in the proper range
        tensor.clamp_(min=a, max=b)
        return tensor


def trunc_normal_(tensor, mean=0.0, std=1.0, a=-2.0, b=2.0):
    # type: (Tensor, float, float, float, float) -> Tensor
    r"""Fills the input Tensor with values drawn from a truncated
    normal distribution. The values are effectively drawn from the
    normal distribution :math:`\mathcal{N}(\text{mean}, \text{std}^2)`
    with values outside :math:`[a, b]` redrawn until they are within
    the bounds. The method used for generating the random values works
    best when :math:`a \leq \text{mean} \leq b`.
    Args:
        tensor: an n-dimensional `torch.Tensor`
        mean: the mean of the normal distribution
        std: the standard deviation of the normal distribution
        a: the minimum cutoff value
        b: the maximum cutoff value
    Examples:
        >>> w = torch.empty(3, 5)
        >>> nn.init.trunc_normal_(w)
    """
    return _no_grad_trunc_normal_(tensor, mean, std, a, b)


def drop_path(x, drop_prob: float = 0.0, training: bool = False):
    """Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).
    This is the same as the DropConnect impl I created for EfficientNet, etc networks, however,
    the original name is misleading as 'Drop Connect' is a different form of dropout in a separate paper...
    See discussion: https://github.com/tensorflow/tpu/issues/494#issuecomment-532968956 ... I've opted for
    changing the layer and argument names to 'drop path' rather than mix DropConnect as a layer name and use
    'survival rate' as the argument.
    """
    if drop_prob == 0.0 or not training:
        return x
    keep_prob = 1 - drop_prob
    shape = (x.shape[0],) + (1,) * (
        x.ndim - 1
    )  # work with diff dim tensors, not just 2D ConvNets
    random_tensor = keep_prob + \
        torch.rand(shape, dtype=x.dtype, device=x.device)
    random_tensor.floor_()  # binarize
    output = x.div(keep_prob) * random_tensor
    return output


class DropPath(nn.Module):
    """Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks)."""

    def __init__(self, drop_prob=None):
        super(DropPath, self).__init__()
        self.drop_prob = drop_prob

    def forward(self, x):
        return drop_path(x, self.drop_prob, self.training)


class Mlp(nn.Module):
    def __init__(
            self,
            abits, 
            wbits, 
            gbits,
            qdtype,
            in_features,
            hidden_features=None,
            out_features=None,
            act_layer=False,
            drop=0.0):
        super().__init__()
        out_features = out_features or in_features
        hidden_features = hidden_features or in_features
        # self.fc1 = nn.Linear(in_features, hidden_features)
        self.fc1 = Quantized_Linear(
                                weight_quantize_module=Quantizer(wbits, qdtype), 
                                act_quantize_module=Quantizer(abits, qdtype), 
                                grad_quantize_module=Quantizer(gbits, qdtype),
                                in_features=in_features, 
                                out_features=hidden_features, 
                                bias=True
                                )

        self.act = act_layer

        self.qact1 = QuantAct(abits, qdtype)
        # self.fc2 = nn.Linear(hidden_features, out_features)
        self.fc2 = Quantized_Linear(
                                weight_quantize_module=Quantizer(wbits, qdtype), 
                                act_quantize_module=Quantizer(abits, qdtype), 
                                grad_quantize_module=Quantizer(gbits, qdtype),
                                in_features=hidden_features, 
                                out_features=out_features, 
                                bias=True
                                )
        # self.qact2 = QuantAct(abits, qdtype)
        # self.drop = nn.Dropout(drop)

        # self.qact_gelu = QuantAct()

    def forward(self, x, act_scaling_factor):
        x = self.fc1(x, act_scaling_factor)
        # print("1", x.shape)
        # x, act_scaling_factor = self.qact_gelu(x, act_scaling_factor)
        x = self.act(x)
        # print("2", x.shape)
        x, act_scaling_factor = self.qact1(x)
        # x = self.drop(x)
        x = self.fc2(x, act_scaling_factor)
        # print("3", x.shape)
        # x, act_scaling_factor = self.qact2(x, act_scaling_factor)
        # x = self.drop(x)
        return x


# class PatchEmbed(nn.Module):
#     """Image to Patch Embedding"""

#     def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768, norm_layer=None):
#         super().__init__()
#         img_size = to_2tuple(img_size)
#         patch_size = to_2tuple(patch_size)
#         self.img_size = img_size
#         self.patch_size = patch_size

#         self.grid_size = (img_size[0] // patch_size[0],
#                           img_size[1] // patch_size[1])
#         self.num_patches = self.grid_size[0] * self.grid_size[1]

#         self.norm_layer = norm_layer

#         # self.proj = QuantConv2d(
#         #     in_chans,
#         #     embed_dim,
#         #     kernel_size=patch_size,
#         #     stride=patch_size,
#         # )

#         self.proj = nn.Conv2d(
#             in_chans,
#             embed_dim,
#             kernel_size=patch_size,
#             stride=patch_size,
#         )

#         # if self.norm_layer:
#         #     self.qact_before_norm = QuantAct()
#         #     self.norm = norm_layer(embed_dim)
#         # self.qact = QuantAct(16)


#     # def forward(self, x, act_scaling_factor):
#     #     B, C, H, W = x.shape
#     #     # FIXME look at relaxing size constraints
#     #     assert (
#     #         H == self.img_size[0] and W == self.img_size[1]
#     #     ), f"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]})."
#     #     x, act_scaling_factor = self.proj(x, act_scaling_factor)
#     #     x = x.flatten(2).transpose(1, 2)
#     #     if self.norm_layer:
#     #         x, act_scaling_factor = self.qact_before_norm(x, act_scaling_factor)
#     #         x, act_scaling_factor = self.norm(x, act_scaling_factor)
#     #     x, act_scaling_factor = self.qact(x, act_scaling_factor)
#     #     return x, act_scaling_factor

#     def forward(self, x):
#         B, C, H, W = x.shape
#         # FIXME look at relaxing size constraints
#         assert H == self.img_size[0] and W == self.img_size[1], \
#             f"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]})."
#         x = self.proj(x).flatten(2).transpose(1, 2)
#         # x, act_scaling_factor = self.qact(x, act_scaling_factor)
#         return x

class PatchEmbed(nn.Module):
    """ 2D Image to Patch Embedding
    """
    # output_fmt: Format
    # dynamic_img_pad: torch.jit.Final[bool]

    def __init__(
            self,
            img_size= 224,
            patch_size= 16,
            in_chans = 3,
            embed_dim = 768,
            norm_layer = None,
            flatten= True,
            output_fmt = None,
            bias = True,
            strict_img_size= True,
            dynamic_img_pad = False,
    ):
        super().__init__()
        self.patch_size = to_2tuple(patch_size)
        self.img_size, self.grid_size, self.num_patches = self._init_img_size(img_size)

        if output_fmt is not None:
            self.flatten = False
            # self.output_fmt = Format(output_fmt)
        else:
            # flatten spatial dim and transpose to channels last, kept for bwd compat
            self.flatten = flatten
            # self.output_fmt = Format.NCHW
        self.strict_img_size = strict_img_size
        # self.dynamic_img_pad = dynamic_img_pad

        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size, bias=bias)
        self.norm = norm_layer(embed_dim) if norm_layer else nn.Identity()

    def _init_img_size(self, img_size: Union[int, Tuple[int, int]]):
        assert self.patch_size
        if img_size is None:
            return None, None, None
        img_size = to_2tuple(img_size)
        grid_size = tuple([s // p for s, p in zip(img_size, self.patch_size)])
        num_patches = grid_size[0] * grid_size[1]
        return img_size, grid_size, num_patches

    def set_input_size(
            self,
            img_size: Optional[Union[int, Tuple[int, int]]] = None,
            patch_size: Optional[Union[int, Tuple[int, int]]] = None,
    ):
        new_patch_size = None
        if patch_size is not None:
            new_patch_size = to_2tuple(patch_size)
        if new_patch_size is not None and new_patch_size != self.patch_size:
            with torch.no_grad():
                new_proj = nn.Conv2d(
                    self.proj.in_channels,
                    self.proj.out_channels,
                    kernel_size=new_patch_size,
                    stride=new_patch_size,
                    bias=self.proj.bias is not None,
                )
                new_proj.weight.copy_(resample_patch_embed(self.proj.weight, new_patch_size, verbose=True))
                if self.proj.bias is not None:
                    new_proj.bias.copy_(self.proj.bias)
                self.proj = new_proj
            self.patch_size = new_patch_size
        img_size = img_size or self.img_size
        if img_size != self.img_size or new_patch_size is not None:
            self.img_size, self.grid_size, self.num_patches = self._init_img_size(img_size)

    def feat_ratio(self, as_scalar=True) -> Union[Tuple[int, int], int]:
        if as_scalar:
            return max(self.patch_size)
        else:
            return self.patch_size

    def dynamic_feat_size(self, img_size: Tuple[int, int]) -> Tuple[int, int]:
        """ Get grid (feature) size for given image size taking account of dynamic padding.
        NOTE: must be torchscript compatible so using fixed tuple indexing
        """

        return img_size[0] // self.patch_size[0], img_size[1] // self.patch_size[1]

    def forward(self, x):
        B, C, H, W = x.shape
        # if self.img_size is not None:
        #     if self.strict_img_size:
        #         # _assert(H == self.img_size[0], f"Input height ({H}) doesn't match model ({self.img_size[0]}).")
        #         # _assert(W == self.img_size[1], f"Input width ({W}) doesn't match model ({self.img_size[1]}).")
        #     elif not self.dynamic_img_pad:
        #         _assert(
        #             H % self.patch_size[0] == 0,
        #             f"Input height ({H}) should be divisible by patch size ({self.patch_size[0]})."
        #         )
        #         _assert(
        #             W % self.patch_size[1] == 0,
        #             f"Input width ({W}) should be divisible by patch size ({self.patch_size[1]})."
        #         )
        # if self.dynamic_img_pad:
        #     pad_h = (self.patch_size[0] - H % self.patch_size[0]) % self.patch_size[0]
        #     pad_w = (self.patch_size[1] - W % self.patch_size[1]) % self.patch_size[1]
        #     x = F.pad(x, (0, pad_w, 0, pad_h))
        x = self.proj(x)
        if self.flatten:
            x = x.flatten(2).transpose(1, 2)  # NCHW -> NLC
        # elif self.output_fmt != Format.NCHW:
        #     x = nchw_to(x, self.output_fmt)
        x = self.norm(x)
        return x